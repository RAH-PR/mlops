{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import datetime\n",
    "import pytorch_lightning as L\n",
    "\n",
    "from glue_transfomer import GLUETransformer\n",
    "from data_module import GLUEDataModule\n",
    "from wandb_utils import setup_wandb\n",
    "\n",
    "#from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:True-1e-05-0.1-100000-100000) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">project_1-week_2-use_cyclic_lr-base_lr-max_lr-step_size_up-step_size_down-True-1e-05-0.1-100000-100000-20241112_161044</strong> at: <a href='https://wandb.ai/rara/mlops/runs/True-1e-05-0.1-100000-100000' target=\"_blank\">https://wandb.ai/rara/mlops/runs/True-1e-05-0.1-100000-100000</a><br/> View project at: <a href='https://wandb.ai/rara/mlops' target=\"_blank\">https://wandb.ai/rara/mlops</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>mlops/project_1/week_2/True-1e-05-0.1-100000-100000/logs/wandb/run-20241112_161044-True-1e-05-0.1-100000-100000/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:True-1e-05-0.1-100000-100000). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>mlops/project_1/week_2/True-1e-05-0.1-100000-100000/logs/wandb/run-20241112_161716-True-1e-05-0.1-100000-100000</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rara/mlops/runs/True-1e-05-0.1-100000-100000' target=\"_blank\">project_1-week_2-use_cyclic_lr-base_lr-max_lr-step_size_up-step_size_down-True-1e-05-0.1-100000-100000-20241112_161716</a></strong> to <a href='https://wandb.ai/rara/mlops' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rara/mlops' target=\"_blank\">https://wandb.ai/rara/mlops</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rara/mlops/runs/True-1e-05-0.1-100000-100000' target=\"_blank\">https://wandb.ai/rara/mlops/runs/True-1e-05-0.1-100000-100000</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate' : 1e-5,\n",
    "    'warmup_steps': 0,\n",
    "    'weight_decay': 0.0,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_batch_size': 32,\n",
    "    'use_cyclic_lr': True,\n",
    "    'base_lr': 1e-5,\n",
    "    'max_lr': 1e-1,\n",
    "    'step_size_up': 100000,\n",
    "    'step_size_down': 100000\n",
    "}\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "task_name = 'mrpc'\n",
    "project_name = 'mlops'\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "wandb_logger, checkpoint_callback, log_dir, checkpoint_dir = setup_wandb(hyperparams, model_name, task_name, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2070831101f94f389368491480daa119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mlops/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2829: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa68a2af8d244edb85dab161db3242a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/miniconda3/envs/mlops/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96969bb0270484c8205f4f31fc5546d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mlops/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2829: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c396362db15144329dfbc1e5a85488d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095b74e3d2004a7a9dc4529585dbba19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                                | Params | Mode\n",
      "---------------------------------------------------------------------\n",
      "0 | model | DistilBertForSequenceClassification | 67.0 M | eval\n",
      "---------------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "96        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015d7da371a14452a75ede1914850526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mlops/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/opt/miniconda3/envs/mlops/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eda8c962b664f15b8b01ecdbc9f8716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96c5229ba1c4c438a01ebef31fc8547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939d25f4c8fb4db48a20588fbae47001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42085ccf5c674440822e09c40b41a788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a29fe9f0004f75b98557b3b6e7033b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁██</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>f1</td><td>▁▇█</td></tr><tr><td>trainer/global_step</td><td>▁▅█</td></tr><tr><td>training_loss</td><td>█▇▇▇▆█▆█▆▇▆▇▆▆▅▅▅▆▆▅▅▇▅▆▄▄▆▄▃▄▂▄▄▂▁▃▂▃▃▂</td></tr><tr><td>val_loss</td><td>██▄▅▅▅▆▆▆▆▆▅▆▅▅▃▃▄▃▃▅▄▄▄▃▄▅▄▂▃▁▅▅▄▄▃▄▆▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.83578</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>f1</td><td>0.88388</td></tr><tr><td>trainer/global_step</td><td>344</td></tr><tr><td>training_loss</td><td>0.15354</td></tr><tr><td>val_loss</td><td>0.38154</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">project_1-week_2-use_cyclic_lr-base_lr-max_lr-step_size_up-step_size_down-True-1e-05-0.1-100000-100000-20241112_161716</strong> at: <a href='https://wandb.ai/rara/mlops/runs/True-1e-05-0.1-100000-100000' target=\"_blank\">https://wandb.ai/rara/mlops/runs/True-1e-05-0.1-100000-100000</a><br/> View project at: <a href='https://wandb.ai/rara/mlops' target=\"_blank\">https://wandb.ai/rara/mlops</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>mlops/project_1/week_2/True-1e-05-0.1-100000-100000/logs/wandb/run-20241112_161716-True-1e-05-0.1-100000-100000/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L.seed_everything(42)\n",
    "\n",
    "dm = GLUEDataModule(\n",
    "    model_name_or_path=model_name,\n",
    "    task_name=task_name,\n",
    ")\n",
    "dm.setup(\"fit\")\n",
    "model = GLUETransformer(\n",
    "    model_name_or_path=model_name,\n",
    "    num_labels=dm.num_labels,\n",
    "    eval_splits=dm.eval_splits,\n",
    "    task_name=dm.task_name,\n",
    "    **hyperparams\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=10,\n",
    "    default_root_dir=log_dir, \n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "hyperparams = {\n",
    "    'use_cyclic_lr':True,\n",
    "    'base_lr': 1e-5,\n",
    "    'max_lr': 1e-1,\n",
    "    'step_size_up': 100000,\n",
    "    'step_size_down':  100000\n",
    "}\n",
    "\n",
    "task_name = 'mrpc'\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "project_name = 'mlops'\n",
    "\n",
    "hyperparam_names = \"-\".join(hyperparams.keys())\n",
    "hyperparam_values = \"-\".join([str(value) for value in hyperparams.values()])\n",
    "hyperparam_string = f\"{hyperparam_names}/{hyperparam_values}\"\n",
    "\n",
    "experiment_name = f'project_1/week_2/{hyperparam_string}-{timestamp}'\n",
    "folder_structure = f'mlops/project_1/week_2/{hyperparam_string}/'\n",
    "\n",
    "wandb_experiment_name = experiment_name.replace(\"/\", \"-\")\n",
    "wandb_hyperparam_string = hyperparam_string.replace(\"/\", \"-\")\n",
    "\n",
    "epochs = 3  # do not change this\n",
    "\n",
    "log_dir = os.path.join(folder_structure, 'logs')\n",
    "checkpoint_dir = os.path.join(folder_structure, 'checkpoints')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=wandb_experiment_name,\n",
    "    #name = experiment_name,\n",
    "    config={  \n",
    "        **hyperparams,  # Pass all hyperparameters dynamically\n",
    "        \"model_name\": model_name,\n",
    "        \"task_name\": task_name,\n",
    "    },\n",
    "    tags=[task_name, model_name],  # Optional tags\n",
    "    dir=log_dir,  # Use custom log directory for WandB logs\n",
    "    id=f\"{wandb_hyperparam_string}\",  # Custom ID using hyperparameter string\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=project_name,\n",
    "    name=experiment_name,\n",
    "    log_model=True,  # Log model checkpoints to W&B\n",
    "    save_dir=log_dir,  # Path for logs\n",
    "    id=f\"{hyperparam_string}\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  \n",
    "    dirpath=checkpoint_dir, \n",
    "    filename=hyperparam_string + '-{epoch:02d}-{val_loss:.2f}',  \n",
    "    save_top_k=1,  \n",
    "    mode='min',  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_experiment_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
